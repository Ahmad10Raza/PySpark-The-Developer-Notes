{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98ca221a",
   "metadata": {},
   "source": [
    "# Setting up PySpark in Locan Ubuntu System?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05eb02e0",
   "metadata": {},
   "source": [
    "### âœ… Stage 2: Setting Up PySpark in a Local Ubuntu System (Step-by-Step)\n",
    "\n",
    "Hereâ€™s a **clean and professional guide** to set up PySpark on Ubuntu (22.04 or similar) for local development.\n",
    "\n",
    "\n",
    "\n",
    "### ðŸ”§ Prerequisites\n",
    "\n",
    "* âœ… Python (3.7+)\n",
    "* âœ… Java (Java 8 or 11 recommended)\n",
    "* âœ… pip (Python package manager)\n",
    "* âœ… Ubuntu terminal access\n",
    "\n",
    "\n",
    "\n",
    "### ðŸš€ Step-by-Step Installation Guide\n",
    "\n",
    "#### âœ… Step 1: Install Java (if not already installed)\n",
    "\n",
    "```bash\n",
    "sudo apt update\n",
    "sudo apt install openjdk-11-jdk -y\n",
    "```\n",
    "\n",
    "âœ… **Check version**:\n",
    "\n",
    "```bash\n",
    "java -version\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "#### âœ… Step 2: Install Python & pip (if not already)\n",
    "\n",
    "```bash\n",
    "sudo apt install python3 python3-pip -y\n",
    "```\n",
    "\n",
    "âœ… **Check version**:\n",
    "\n",
    "```bash\n",
    "python3 --version\n",
    "pip3 --version\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "#### âœ… Step 3: Install Apache Spark via pip\n",
    "\n",
    "Use `findspark` and `pyspark`:\n",
    "\n",
    "```bash\n",
    "pip3 install pyspark findspark\n",
    "```\n",
    "\n",
    "* **pyspark** â†’ PySpark bindings\n",
    "* **findspark** â†’ Helps Jupyter or Python scripts locate Spark\n",
    "\n",
    "\n",
    "\n",
    "#### âœ… Step 4: Set Environment Variables (optional but recommended)\n",
    "\n",
    "Edit your `.bashrc` file:\n",
    "\n",
    "```bash\n",
    "nano ~/.bashrc\n",
    "```\n",
    "\n",
    "Add these lines at the end:\n",
    "\n",
    "```bash\n",
    "export SPARK_HOME=$(pip3 show pyspark | grep Location | cut -d' ' -f2)/pyspark\n",
    "export PATH=$SPARK_HOME/bin:$PATH\n",
    "```\n",
    "\n",
    "Then run:\n",
    "\n",
    "```bash\n",
    "source ~/.bashrc\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### âœ… Step 5: Test PySpark Shell\n",
    "\n",
    "```bash\n",
    "pyspark\n",
    "```\n",
    "\n",
    "If everything works, you'll see:\n",
    "\n",
    "```\n",
    ">>> Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /___/ .__/\\_,_/_/ /_/\\_\\   version X.X.X\n",
    "      /_/\n",
    "```\n",
    "\n",
    "Type `exit()` or press `Ctrl + D` to exit.\n",
    "\n",
    "\n",
    "\n",
    "#### âœ… Step 6: Test PySpark in Python\n",
    "\n",
    "Create a file `test_spark.py`:\n",
    "\n",
    "```python\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"TestApp\").getOrCreate()\n",
    "\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "df.show()\n",
    "```\n",
    "\n",
    "Run it:\n",
    "\n",
    "```bash\n",
    "python3 test_spark.py\n",
    "```\n",
    "\n",
    "You should see the DataFrame printed in your terminal.\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… (Optional) Step 7: Jupyter Notebook + PySpark\n",
    "\n",
    "Install Jupyter:\n",
    "\n",
    "```bash\n",
    "pip3 install notebook\n",
    "```\n",
    "\n",
    "Then create a kernel:\n",
    "\n",
    "```bash\n",
    "pip3 install ipykernel\n",
    "python3 -m ipykernel install --user --name=pyspark_env\n",
    "```\n",
    "\n",
    "Launch Jupyter:\n",
    "\n",
    "```bash\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "Use this code to initialize Spark in your notebook:\n",
    "\n",
    "```python\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"NotebookApp\").getOrCreate()\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0456d546",
   "metadata": {},
   "source": [
    "# Setting up PySpark in Google Colab?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348ffa6e",
   "metadata": {},
   "source": [
    "### âœ… Setting Up PySpark in Google Colab (2025 Guide)\n",
    "\n",
    "Google Colab is an excellent platform for running **PySpark** without local installation. Follow these simple steps to configure and run Spark on Colab.\n",
    "\n",
    "\n",
    "\n",
    "### ðŸ”¹ Step-by-Step Setup Guide\n",
    "\n",
    "#### âœ… Step 1: Install PySpark\n",
    "\n",
    "Run the following in a Colab cell:\n",
    "\n",
    "```python\n",
    "!apt-get install openjdk-11-jdk -y\n",
    "!pip install pyspark\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "#### âœ… Step 2: Set Environment Variables\n",
    "\n",
    "```python\n",
    "import os\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/usr/local/lib/python3.10/dist-packages/pyspark\"\n",
    "```\n",
    "\n",
    "âœ… This tells Spark where Java is located (required to run on JVM).\n",
    "\n",
    "\n",
    "\n",
    "#### âœ… Step 3: Start Spark Session\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Colab PySpark\") \\\n",
    "    .getOrCreate()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "#### âœ… Step 4: Test Spark\n",
    "\n",
    "```python\n",
    "data = [(\"Ahmad\", 22), (\"Raza\", 25)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "df.show()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### âœ… Optional: Check Spark Version\n",
    "\n",
    "```python\n",
    "print(spark.version)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### ðŸ“Œ Notes\n",
    "\n",
    "* No need for `findspark` in Colab, unless running multiple sessions.\n",
    "* Spark runs **locally** on Colabâ€™s virtual machine (not distributed).\n",
    "* You can upload files to Colab or use `gdown`, `wget`, or mount Google Drive for data input.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56139f2",
   "metadata": {},
   "source": [
    "# Setting up PySpark in Databrics?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ad5cd4",
   "metadata": {},
   "source": [
    "### âœ… Setting Up PySpark in **Databricks**\n",
    "\n",
    "Databricks is a powerful cloud-based platform built on Apache Spark. It offers an easy-to-use environment for running **PySpark** code without manual installation or setup.\n",
    "\n",
    "\n",
    "\n",
    "### ðŸ”¹ Step-by-Step Guide to Set Up PySpark in Databricks\n",
    "\n",
    "#### âœ… Step 1: Create a Databricks Account\n",
    "\n",
    "1. Go to: [https://community.cloud.databricks.com](https://community.cloud.databricks.com)\n",
    "2. Sign up for a **free Community Edition** account (sufficient for learning).\n",
    "\n",
    "\n",
    "\n",
    "#### âœ… Step 2: Create a Workspace\n",
    "\n",
    "Once logged in:\n",
    "\n",
    "1. Go to the **Workspace** tab.\n",
    "2. Click on `Create > Notebook`.\n",
    "\n",
    "\n",
    "\n",
    "#### âœ… Step 3: Create a New Notebook\n",
    "\n",
    "1. **Name** your notebook (e.g., `My PySpark Demo`).\n",
    "2. **Default language**: Select `Python`.\n",
    "3. **Cluster**: Youâ€™ll be prompted to attach a cluster.\n",
    "\n",
    "\n",
    "\n",
    "#### âœ… Step 4: Create and Start a Cluster\n",
    "\n",
    "1. Go to `Compute > Create Cluster`.\n",
    "2. Set:\n",
    "\n",
    "   * Cluster name: `my-cluster`\n",
    "   * Runtime: Choose default (`10.x` or higher is fine)\n",
    "   * Cluster mode: `Single Node`\n",
    "3. Click **Create Cluster**.\n",
    "\n",
    "ðŸ“Œ Wait 2â€“3 minutes for the cluster to initialize.\n",
    "\n",
    "\n",
    "\n",
    "#### âœ… Step 5: Run PySpark Code in Notebook\n",
    "\n",
    "You can now run PySpark like this:\n",
    "\n",
    "```python\n",
    "# Create a SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DatabricksApp\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [(\"Ahmad\", 22), (\"Raza\", 25)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "df.show()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### ðŸ”„ Bonus Features in Databricks\n",
    "\n",
    "| Feature                    | Use                                   |\n",
    "| -------------------------- | ------------------------------------- |\n",
    "| ðŸ“Š Built-in visualizations | Click on result âž Visualize           |\n",
    "| ðŸ“ File system (DBFS)      | `/dbfs/` path for storing files       |\n",
    "| ðŸ“š Markdown Support        | `%md` cells for documentation         |\n",
    "| ðŸ“¦ Libraries               | Install via `Libraries > Install New` |\n",
    "\n",
    "\n",
    "\n",
    "### âœ… Databricks Magic Commands\n",
    "\n",
    "| Magic Command | Description                          |\n",
    "| ------------- | ------------------------------------ |\n",
    "| `%fs`         | Access Databricks File System (DBFS) |\n",
    "| `%run`        | Run another notebook                 |\n",
    "| `%sql`        | Run SQL queries                      |\n",
    "| `%python`     | Run Python code (default)            |\n",
    "| `%sh`         | Run shell commands                   |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3881736",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
