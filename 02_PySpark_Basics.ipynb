{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98ca221a",
   "metadata": {},
   "source": [
    "# Setting up PySpark in Locan Ubuntu System?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05eb02e0",
   "metadata": {},
   "source": [
    "### ✅ Stage 2: Setting Up PySpark in a Local Ubuntu System (Step-by-Step)\n",
    "\n",
    "Here’s a **clean and professional guide** to set up PySpark on Ubuntu (22.04 or similar) for local development.\n",
    "\n",
    "\n",
    "\n",
    "### 🔧 Prerequisites\n",
    "\n",
    "* ✅ Python (3.7+)\n",
    "* ✅ Java (Java 8 or 11 recommended)\n",
    "* ✅ pip (Python package manager)\n",
    "* ✅ Ubuntu terminal access\n",
    "\n",
    "\n",
    "\n",
    "### 🚀 Step-by-Step Installation Guide\n",
    "\n",
    "#### ✅ Step 1: Install Java (if not already installed)\n",
    "\n",
    "```bash\n",
    "sudo apt update\n",
    "sudo apt install openjdk-11-jdk -y\n",
    "```\n",
    "\n",
    "✅ **Check version**:\n",
    "\n",
    "```bash\n",
    "java -version\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "#### ✅ Step 2: Install Python & pip (if not already)\n",
    "\n",
    "```bash\n",
    "sudo apt install python3 python3-pip -y\n",
    "```\n",
    "\n",
    "✅ **Check version**:\n",
    "\n",
    "```bash\n",
    "python3 --version\n",
    "pip3 --version\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "#### ✅ Step 3: Install Apache Spark via pip\n",
    "\n",
    "Use `findspark` and `pyspark`:\n",
    "\n",
    "```bash\n",
    "pip3 install pyspark findspark\n",
    "```\n",
    "\n",
    "* **pyspark** → PySpark bindings\n",
    "* **findspark** → Helps Jupyter or Python scripts locate Spark\n",
    "\n",
    "\n",
    "\n",
    "#### ✅ Step 4: Set Environment Variables (optional but recommended)\n",
    "\n",
    "Edit your `.bashrc` file:\n",
    "\n",
    "```bash\n",
    "nano ~/.bashrc\n",
    "```\n",
    "\n",
    "Add these lines at the end:\n",
    "\n",
    "```bash\n",
    "export SPARK_HOME=$(pip3 show pyspark | grep Location | cut -d' ' -f2)/pyspark\n",
    "export PATH=$SPARK_HOME/bin:$PATH\n",
    "```\n",
    "\n",
    "Then run:\n",
    "\n",
    "```bash\n",
    "source ~/.bashrc\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### ✅ Step 5: Test PySpark Shell\n",
    "\n",
    "```bash\n",
    "pyspark\n",
    "```\n",
    "\n",
    "If everything works, you'll see:\n",
    "\n",
    "```\n",
    ">>> Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /___/ .__/\\_,_/_/ /_/\\_\\   version X.X.X\n",
    "      /_/\n",
    "```\n",
    "\n",
    "Type `exit()` or press `Ctrl + D` to exit.\n",
    "\n",
    "\n",
    "\n",
    "#### ✅ Step 6: Test PySpark in Python\n",
    "\n",
    "Create a file `test_spark.py`:\n",
    "\n",
    "```python\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"TestApp\").getOrCreate()\n",
    "\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "df.show()\n",
    "```\n",
    "\n",
    "Run it:\n",
    "\n",
    "```bash\n",
    "python3 test_spark.py\n",
    "```\n",
    "\n",
    "You should see the DataFrame printed in your terminal.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ (Optional) Step 7: Jupyter Notebook + PySpark\n",
    "\n",
    "Install Jupyter:\n",
    "\n",
    "```bash\n",
    "pip3 install notebook\n",
    "```\n",
    "\n",
    "Then create a kernel:\n",
    "\n",
    "```bash\n",
    "pip3 install ipykernel\n",
    "python3 -m ipykernel install --user --name=pyspark_env\n",
    "```\n",
    "\n",
    "Launch Jupyter:\n",
    "\n",
    "```bash\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "Use this code to initialize Spark in your notebook:\n",
    "\n",
    "```python\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"NotebookApp\").getOrCreate()\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0456d546",
   "metadata": {},
   "source": [
    "# Setting up PySpark in Google Colab?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348ffa6e",
   "metadata": {},
   "source": [
    "### ✅ Setting Up PySpark in Google Colab (2025 Guide)\n",
    "\n",
    "Google Colab is an excellent platform for running **PySpark** without local installation. Follow these simple steps to configure and run Spark on Colab.\n",
    "\n",
    "\n",
    "\n",
    "### 🔹 Step-by-Step Setup Guide\n",
    "\n",
    "#### ✅ Step 1: Install PySpark\n",
    "\n",
    "Run the following in a Colab cell:\n",
    "\n",
    "```python\n",
    "!apt-get install openjdk-11-jdk -y\n",
    "!pip install pyspark\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "#### ✅ Step 2: Set Environment Variables\n",
    "\n",
    "```python\n",
    "import os\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/usr/local/lib/python3.10/dist-packages/pyspark\"\n",
    "```\n",
    "\n",
    "✅ This tells Spark where Java is located (required to run on JVM).\n",
    "\n",
    "\n",
    "\n",
    "#### ✅ Step 3: Start Spark Session\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Colab PySpark\") \\\n",
    "    .getOrCreate()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "#### ✅ Step 4: Test Spark\n",
    "\n",
    "```python\n",
    "data = [(\"Ahmad\", 22), (\"Raza\", 25)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "df.show()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### ✅ Optional: Check Spark Version\n",
    "\n",
    "```python\n",
    "print(spark.version)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 📌 Notes\n",
    "\n",
    "* No need for `findspark` in Colab, unless running multiple sessions.\n",
    "* Spark runs **locally** on Colab’s virtual machine (not distributed).\n",
    "* You can upload files to Colab or use `gdown`, `wget`, or mount Google Drive for data input.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56139f2",
   "metadata": {},
   "source": [
    "# Setting up PySpark in Databrics?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ad5cd4",
   "metadata": {},
   "source": [
    "### ✅ Setting Up PySpark in **Databricks**\n",
    "\n",
    "Databricks is a powerful cloud-based platform built on Apache Spark. It offers an easy-to-use environment for running **PySpark** code without manual installation or setup.\n",
    "\n",
    "\n",
    "\n",
    "### 🔹 Step-by-Step Guide to Set Up PySpark in Databricks\n",
    "\n",
    "#### ✅ Step 1: Create a Databricks Account\n",
    "\n",
    "1. Go to: [https://community.cloud.databricks.com](https://community.cloud.databricks.com)\n",
    "2. Sign up for a **free Community Edition** account (sufficient for learning).\n",
    "\n",
    "\n",
    "\n",
    "#### ✅ Step 2: Create a Workspace\n",
    "\n",
    "Once logged in:\n",
    "\n",
    "1. Go to the **Workspace** tab.\n",
    "2. Click on `Create > Notebook`.\n",
    "\n",
    "\n",
    "\n",
    "#### ✅ Step 3: Create a New Notebook\n",
    "\n",
    "1. **Name** your notebook (e.g., `My PySpark Demo`).\n",
    "2. **Default language**: Select `Python`.\n",
    "3. **Cluster**: You’ll be prompted to attach a cluster.\n",
    "\n",
    "\n",
    "\n",
    "#### ✅ Step 4: Create and Start a Cluster\n",
    "\n",
    "1. Go to `Compute > Create Cluster`.\n",
    "2. Set:\n",
    "\n",
    "   * Cluster name: `my-cluster`\n",
    "   * Runtime: Choose default (`10.x` or higher is fine)\n",
    "   * Cluster mode: `Single Node`\n",
    "3. Click **Create Cluster**.\n",
    "\n",
    "📌 Wait 2–3 minutes for the cluster to initialize.\n",
    "\n",
    "\n",
    "\n",
    "#### ✅ Step 5: Run PySpark Code in Notebook\n",
    "\n",
    "You can now run PySpark like this:\n",
    "\n",
    "```python\n",
    "# Create a SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DatabricksApp\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [(\"Ahmad\", 22), (\"Raza\", 25)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "df.show()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 🔄 Bonus Features in Databricks\n",
    "\n",
    "| Feature                    | Use                                   |\n",
    "| -------------------------- | ------------------------------------- |\n",
    "| 📊 Built-in visualizations | Click on result ➝ Visualize           |\n",
    "| 📁 File system (DBFS)      | `/dbfs/` path for storing files       |\n",
    "| 📚 Markdown Support        | `%md` cells for documentation         |\n",
    "| 📦 Libraries               | Install via `Libraries > Install New` |\n",
    "\n",
    "\n",
    "\n",
    "### ✅ Databricks Magic Commands\n",
    "\n",
    "| Magic Command | Description                          |\n",
    "| ------------- | ------------------------------------ |\n",
    "| `%fs`         | Access Databricks File System (DBFS) |\n",
    "| `%run`        | Run another notebook                 |\n",
    "| `%sql`        | Run SQL queries                      |\n",
    "| `%python`     | Run Python code (default)            |\n",
    "| `%sh`         | Run shell commands                   |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca97904",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdbaa29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"NotebookApp\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fd130a",
   "metadata": {},
   "source": [
    "# SparkSession and SparkContext?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02858ed0",
   "metadata": {},
   "source": [
    "Sure! Here's the restored content followed by the explanation of **`SparkSession`** and **`SparkContext`** in PySpark.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Setting Up PySpark in Databricks\n",
    "\n",
    "Databricks is a powerful cloud-based platform built on Apache Spark. It offers an easy-to-use environment for running **PySpark** code without manual installation or setup.\n",
    "\n",
    "\n",
    "\n",
    "### 🔹 Step-by-Step Guide to Set Up PySpark in Databricks\n",
    "\n",
    "#### ✅ Step 1: Create a Databricks Account\n",
    "\n",
    "1. Go to: [https://community.cloud.databricks.com](https://community.cloud.databricks.com)\n",
    "2. Sign up for a **free Community Edition** account (sufficient for learning).\n",
    "\n",
    "\n",
    "\n",
    "#### ✅ Step 2: Create a Workspace\n",
    "\n",
    "Once logged in:\n",
    "\n",
    "1. Go to the **Workspace** tab.\n",
    "2. Click on `Create > Notebook`.\n",
    "\n",
    "\n",
    "\n",
    "#### ✅ Step 3: Create a New Notebook\n",
    "\n",
    "1. **Name** your notebook (e.g., `My PySpark Demo`).\n",
    "2. **Default language**: Select `Python`.\n",
    "3. **Cluster**: You’ll be prompted to attach a cluster.\n",
    "\n",
    "\n",
    "\n",
    "#### ✅ Step 4: Create and Start a Cluster\n",
    "\n",
    "1. Go to `Compute > Create Cluster`.\n",
    "2. Set:\n",
    "\n",
    "   * Cluster name: `my-cluster`\n",
    "   * Runtime: Choose default (`10.x` or higher is fine)\n",
    "   * Cluster mode: `Single Node`\n",
    "3. Click **Create Cluster**.\n",
    "\n",
    "📌 Wait 2–3 minutes for the cluster to initialize.\n",
    "\n",
    "\n",
    "\n",
    "#### ✅ Step 5: Run PySpark Code in Notebook\n",
    "\n",
    "You can now run PySpark like this:\n",
    "\n",
    "```python\n",
    "# Create a SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DatabricksApp\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [(\"Ahmad\", 22), (\"Raza\", 25)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "df.show()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 🔄 Bonus Features in Databricks\n",
    "\n",
    "| Feature                    | Use                                   |\n",
    "| -------------------------- | ------------------------------------- |\n",
    "| 📊 Built-in visualizations | Click on result ➝ Visualize           |\n",
    "| 📁 File system (DBFS)      | `/dbfs/` path for storing files       |\n",
    "| 📚 Markdown Support        | `%md` cells for documentation         |\n",
    "| 📦 Libraries               | Install via `Libraries > Install New` |\n",
    "\n",
    "\n",
    "\n",
    "### ✅ Databricks Magic Commands\n",
    "\n",
    "| Magic Command | Description                          |\n",
    "| ------------- | ------------------------------------ |\n",
    "| `%fs`         | Access Databricks File System (DBFS) |\n",
    "| `%run`        | Run another notebook                 |\n",
    "| `%sql`        | Run SQL queries                      |\n",
    "| `%python`     | Run Python code (default)            |\n",
    "| `%sh`         | Run shell commands                   |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2579712b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 🔍 Now: SparkSession vs SparkContext in PySpark\n",
    "\n",
    "### ✅ 1. What is `SparkSession`?\n",
    "\n",
    "**`SparkSession`** is the entry point to work with Spark functionality using the **DataFrame and SQL API** in PySpark.\n",
    "\n",
    "It combines `SQLContext`, `HiveContext`, and `SparkContext` into one unified object.\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .getOrCreate()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### ✅ 2. What is `SparkContext`?\n",
    "\n",
    "**`SparkContext`** is the original entry point in older versions of Spark (< 2.0). It allows interaction with Spark Core and RDDs.\n",
    "\n",
    "```python\n",
    "sc = spark.sparkContext  # Access from SparkSession\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 🆚 Difference Between `SparkSession` and `SparkContext`\n",
    "\n",
    "| Feature       | `SparkSession`                        | `SparkContext`                   |\n",
    "| ------------- | ------------------------------------- | -------------------------------- |\n",
    "| Introduced In | Spark 2.0+                            | Spark 1.x                        |\n",
    "| Used For      | DataFrames, Datasets, SQL, Spark Core | Spark Core and RDDs only         |\n",
    "| Combines      | SQLContext, HiveContext, SparkContext | Only provides core functionality |\n",
    "| Accessed From | `SparkSession.builder.getOrCreate()`  | `spark.sparkContext`             |\n",
    "| Suitable For  | Most use cases today                  | Legacy RDD-based apps            |\n",
    "\n",
    "\n",
    "\n",
    "### ✅ Typical Usage Example\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.appName(\"Example\").getOrCreate()\n",
    "\n",
    "# Access SparkContext\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Use SparkContext to create an RDD\n",
    "rdd = sc.parallelize([1, 2, 3, 4])\n",
    "print(rdd.collect())\n",
    "\n",
    "# Use SparkSession to create a DataFrame\n",
    "df = spark.createDataFrame([(1, \"Ahmad\"), (2, \"Raza\")], [\"ID\", \"Name\"])\n",
    "df.show()\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b534eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/11 15:50:17 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| ID| Name|\n",
      "+---+-----+\n",
      "|  1|Ahmad|\n",
      "|  2| Raza|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.appName(\"Example\").getOrCreate()\n",
    "\n",
    "# Access SparkContext\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Use SparkContext to create an RDD\n",
    "rdd = sc.parallelize([1, 2, 3, 4])\n",
    "print(rdd.collect())\n",
    "\n",
    "# Use SparkSession to create a DataFrame\n",
    "df = spark.createDataFrame([(1, \"Ahmad\"), (2, \"Raza\")], [\"ID\", \"Name\"])\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665ef196",
   "metadata": {},
   "source": [
    "# RDDs (Resilient Distributed Datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0305fefe",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## ✅ **RDDs (Resilient Distributed Datasets)**\n",
    "\n",
    "### 📘 What is an RDD?\n",
    "\n",
    "An **RDD (Resilient Distributed Dataset)** is the **core data structure** of Apache Spark.\n",
    "It is:\n",
    "\n",
    "* **Immutable** (once created, cannot be changed)\n",
    "* **Distributed** across a cluster\n",
    "* **Lazy-evaluated** (transformations are not executed until an action is called)\n",
    "* **Fault-tolerant** (can recover from node failures)\n",
    "\n",
    "\n",
    "\n",
    "### 📌 How to Create RDDs\n",
    "\n",
    "#### 1. From an existing collection (list, tuple):\n",
    "\n",
    "```python\n",
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = sc.parallelize(data)\n",
    "```\n",
    "\n",
    "#### 2. From an external source (e.g., text file):\n",
    "\n",
    "```python\n",
    "rdd = sc.textFile(\"/path/to/file.txt\")\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## 🔧 RDD Transformations vs Actions\n",
    "\n",
    "| Type               | Meaning                                      | Example                |\n",
    "| ------------------ | -------------------------------------------- | ---------------------- |\n",
    "| **Transformation** | Lazy operation, returns a new RDD            | `map()`, `filter()`    |\n",
    "| **Action**         | Triggers execution, returns result to driver | `collect()`, `count()` |\n",
    "\n",
    "\n",
    "\n",
    "## 🔁 **RDD Transformations** (Lazy)\n",
    "\n",
    "These return a new RDD and are lazily evaluated.\n",
    "\n",
    "| Transformation   | Description                        | Example                             |\n",
    "| ---------------- | ---------------------------------- | ----------------------------------- |\n",
    "| `map(func)`      | Applies a function to each element | `rdd.map(lambda x: x * 2)`          |\n",
    "| `filter(func)`   | Filters elements                   | `rdd.filter(lambda x: x > 3)`       |\n",
    "| `flatMap(func)`  | Like map, but flattens result      | `rdd.flatMap(lambda x: x.split())`  |\n",
    "| `distinct()`     | Removes duplicate elements         | `rdd.distinct()`                    |\n",
    "| `union(rdd2)`    | Combines two RDDs                  | `rdd.union(other_rdd)`              |\n",
    "| `intersection()` | Returns common elements            | `rdd1.intersection(rdd2)`           |\n",
    "| `sample()`       | Random sampling of RDD             | `rdd.sample(False, 0.5)`            |\n",
    "| `groupByKey()`   | Groups values with the same key    | Only for (key, value) RDDs          |\n",
    "| `reduceByKey()`  | Aggregates by key                  | `rdd.reduceByKey(lambda x, y: x+y)` |\n",
    "| `sortBy()`       | Sorts by a custom function         | `rdd.sortBy(lambda x: x)`           |\n",
    "\n",
    "\n",
    "\n",
    "## ⚡ **RDD Actions** (Trigger Execution)\n",
    "\n",
    "These return values or output to the driver or external storage.\n",
    "\n",
    "| Action             | Description                             | Example                       |\n",
    "| ------------------ | --------------------------------------- | ----------------------------- |\n",
    "| `collect()`        | Returns all elements as a list          | `rdd.collect()`               |\n",
    "| `count()`          | Returns number of elements              | `rdd.count()`                 |\n",
    "| `first()`          | Returns first element                   | `rdd.first()`                 |\n",
    "| `take(n)`          | Returns first `n` elements              | `rdd.take(3)`                 |\n",
    "| `reduce(func)`     | Reduces elements using function         | `rdd.reduce(lambda x,y: x+y)` |\n",
    "| `saveAsTextFile()` | Saves RDD to file                       | `rdd.saveAsTextFile(\"/out\")`  |\n",
    "| `countByValue()`   | Counts occurrences of each unique value | `rdd.countByValue()`          |\n",
    "| `foreach(func)`    | Applies a function (no return)          | `rdd.foreach(print)`          |\n",
    "\n",
    "\n",
    "\n",
    "## ✅ Example Code: Basic RDD Operations\n",
    "\n",
    "```python\n",
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# Transformations\n",
    "squared_rdd = rdd.map(lambda x: x ** 2)\n",
    "filtered_rdd = squared_rdd.filter(lambda x: x > 10)\n",
    "\n",
    "# Actions\n",
    "print(\"Squared values:\", squared_rdd.collect())\n",
    "print(\"Filtered values:\", filtered_rdd.collect())\n",
    "print(\"Sum of all values:\", rdd.reduce(lambda x, y: x + y))\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 🧠 Why Use RDDs?\n",
    "\n",
    "* Full control over **low-level transformations**\n",
    "* Ideal for **unstructured** or **semi-structured** data\n",
    "* More **manual**, but more **flexible** than DataFrames\n",
    "\n",
    "\n",
    "\n",
    "### ⚠️ When **NOT** to use RDDs:\n",
    "\n",
    "* When working with **structured data** (prefer DataFrames)\n",
    "* When performance and optimization are crucial (RDDs don’t get Catalyst optimizer benefits)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14ca3505",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Squared values: [1, 4, 9, 16, 25]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered values: [16, 25]\n",
      "Sum of all values: 15\n"
     ]
    }
   ],
   "source": [
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# Transformations\n",
    "squared_rdd = rdd.map(lambda x: x ** 2)\n",
    "filtered_rdd = squared_rdd.filter(lambda x: x > 10)\n",
    "\n",
    "# Actions\n",
    "print(\"Squared values:\", squared_rdd.collect())\n",
    "print(\"Filtered values:\", filtered_rdd.collect())\n",
    "print(\"Sum of all values:\", rdd.reduce(lambda x, y: x + y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a3566a",
   "metadata": {},
   "source": [
    "\n",
    "## ✅ Setup (Run this first)\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder.appName(\"RDDFunctions\").getOrCreate()\n",
    "\n",
    "# Get SparkContext\n",
    "sc = spark.sparkContext\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🔁 RDD **Transformations** with Code Examples\n",
    "\n",
    "### 1. `map(func)`\n",
    "\n",
    "```python\n",
    "rdd = sc.parallelize([1, 2, 3])\n",
    "mapped = rdd.map(lambda x: x * 2)\n",
    "print(mapped.collect())  # [2, 4, 6]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2. `filter(func)`\n",
    "\n",
    "```python\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "filtered = rdd.filter(lambda x: x > 3)\n",
    "print(filtered.collect())  # [4, 5]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. `flatMap(func)`\n",
    "\n",
    "```python\n",
    "rdd = sc.parallelize([\"Hello Spark\", \"RDD example\"])\n",
    "flat_mapped = rdd.flatMap(lambda x: x.split())\n",
    "print(flat_mapped.collect())  # ['Hello', 'Spark', 'RDD', 'example']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. `distinct()`\n",
    "\n",
    "```python\n",
    "rdd = sc.parallelize([1, 2, 2, 3, 3, 3])\n",
    "distinct_rdd = rdd.distinct()\n",
    "print(distinct_rdd.collect())  # [1, 2, 3]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5. `union(rdd2)`\n",
    "\n",
    "```python\n",
    "rdd1 = sc.parallelize([1, 2])\n",
    "rdd2 = sc.parallelize([3, 4])\n",
    "unioned = rdd1.union(rdd2)\n",
    "print(unioned.collect())  # [1, 2, 3, 4]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 6. `intersection(rdd2)`\n",
    "\n",
    "```python\n",
    "rdd1 = sc.parallelize([1, 2, 3])\n",
    "rdd2 = sc.parallelize([2, 3, 4])\n",
    "intersected = rdd1.intersection(rdd2)\n",
    "print(intersected.collect())  # [2, 3]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 7. `sample(withReplacement, fraction)`\n",
    "\n",
    "```python\n",
    "rdd = sc.parallelize(range(10))\n",
    "sampled = rdd.sample(False, 0.4)\n",
    "print(sampled.collect())  # Random 40% sample\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 8. `groupByKey()`\n",
    "\n",
    "```python\n",
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 2), (\"a\", 3)])\n",
    "grouped = rdd.groupByKey()\n",
    "print([(k, list(v)) for k, v in grouped.collect()])  # [('a', [1, 3]), ('b', [2])]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 9. `reduceByKey(func)`\n",
    "\n",
    "```python\n",
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 2), (\"a\", 3)])\n",
    "reduced = rdd.reduceByKey(lambda x, y: x + y)\n",
    "print(reduced.collect())  # [('a', 4), ('b', 2)]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 10. `sortBy(func)`\n",
    "\n",
    "```python\n",
    "rdd = sc.parallelize([(\"a\", 3), (\"b\", 1), (\"c\", 2)])\n",
    "sorted_rdd = rdd.sortBy(lambda x: x[1])\n",
    "print(sorted_rdd.collect())  # [('b', 1), ('c', 2), ('a', 3)]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ⚡ RDD **Actions** with Code Examples\n",
    "\n",
    "### 1. `collect()`\n",
    "\n",
    "```python\n",
    "rdd = sc.parallelize([10, 20])\n",
    "print(rdd.collect())  # [10, 20]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2. `count()`\n",
    "\n",
    "```python\n",
    "rdd = sc.parallelize([10, 20, 30])\n",
    "print(rdd.count())  # 3\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. `first()`\n",
    "\n",
    "```python\n",
    "rdd = sc.parallelize([5, 6, 7])\n",
    "print(rdd.first())  # 5\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. `take(n)`\n",
    "\n",
    "```python\n",
    "rdd = sc.parallelize([5, 6, 7, 8])\n",
    "print(rdd.take(2))  # [5, 6]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5. `reduce(func)`\n",
    "\n",
    "```python\n",
    "rdd = sc.parallelize([1, 2, 3, 4])\n",
    "result = rdd.reduce(lambda x, y: x + y)\n",
    "print(result)  # 10\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 6. `saveAsTextFile(path)`\n",
    "\n",
    "```python\n",
    "rdd = sc.parallelize([\"line1\", \"line2\"])\n",
    "rdd.saveAsTextFile(\"/tmp/output_text\")\n",
    "```\n",
    "\n",
    "📌 Run this only in local environments, not in Colab.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. `countByValue()`\n",
    "\n",
    "```python\n",
    "rdd = sc.parallelize([1, 2, 2, 3, 3, 3])\n",
    "print(rdd.countByValue())  # {1: 1, 2: 2, 3: 3}\n",
    "```\n",
    "---\n",
    "\n",
    "### 8. `foreach(func)`\n",
    "\n",
    "```python\n",
    "rdd = sc.parallelize([\"a\", \"b\", \"c\"])\n",
    "rdd.foreach(lambda x: print(\"Letter:\", x))  # Printed in executors\n",
    "```\n",
    "\n",
    "⚠️ In local mode, this may not print in order or at all.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec3d003",
   "metadata": {},
   "source": [
    "# DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa4ae80",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## ✅ **DataFrames in PySpark**\n",
    "\n",
    "### 📘 What is a DataFrame?\n",
    "\n",
    "A **DataFrame** in PySpark is a **distributed collection of data** organized into **named columns**, just like a table in a relational database or a **pandas DataFrame**.\n",
    "\n",
    "It is built **on top of RDDs** and uses **Catalyst optimizer** for query optimization and **Tungsten** for execution, making it faster and more efficient.\n",
    "\n",
    "\n",
    "\n",
    "### 🔍 Key Features of DataFrames\n",
    "\n",
    "* Schema (column names and types)\n",
    "* SQL-like operations (`select`, `filter`, `groupBy`, etc.)\n",
    "* Automatic optimization via Catalyst engine\n",
    "* Lazy evaluation\n",
    "* Supports reading from multiple sources (CSV, JSON, Parquet, Hive, etc.)\n",
    "* Interoperable with SQL\n",
    "\n",
    "\n",
    "\n",
    "## 🔧 Creating a DataFrame\n",
    "\n",
    "### 1. ✅ From a Python List (with schema):\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DataFrameExample\").getOrCreate()\n",
    "\n",
    "data = [(\"Ahmad\", 22), (\"Raza\", 25), (\"Ali\", 30)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "```\n",
    "\n",
    "```\n",
    "+-----+---+\n",
    "| Name|Age|\n",
    "+-----+---+\n",
    "|Ahmad| 22|\n",
    "| Raza| 25|\n",
    "|  Ali| 30|\n",
    "+-----+---+\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 2. ✅ From a CSV File\n",
    "\n",
    "```python\n",
    "df = spark.read.csv(\"data.csv\", header=True, inferSchema=True)\n",
    "df.show()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 3. ✅ From an RDD\n",
    "\n",
    "```python\n",
    "rdd = spark.sparkContext.parallelize([(1, \"Spark\"), (2, \"PySpark\")])\n",
    "df = rdd.toDF([\"ID\", \"Course\"])\n",
    "df.show()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## 🔍 Common DataFrame Operations\n",
    "\n",
    "### ✅ Viewing Data\n",
    "\n",
    "```python\n",
    "df.show()            # Displays rows in tabular format\n",
    "df.printSchema()     # Shows the schema of DataFrame\n",
    "df.columns           # List of column names\n",
    "df.describe().show() # Summary statistics\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### ✅ Selecting Columns\n",
    "\n",
    "```python\n",
    "df.select(\"Name\").show()\n",
    "df.select(\"Name\", \"Age\").show()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### ✅ Filtering Rows\n",
    "\n",
    "```python\n",
    "df.filter(df.Age > 24).show()\n",
    "df.where(df.Name == \"Ahmad\").show()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### ✅ Adding New Columns\n",
    "\n",
    "```python\n",
    "df.withColumn(\"AgePlusOne\", df.Age + 1).show()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### ✅ Renaming Columns\n",
    "\n",
    "```python\n",
    "df.withColumnRenamed(\"Age\", \"NewAge\").show()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### ✅ Dropping Columns\n",
    "\n",
    "```python\n",
    "df.drop(\"Age\").show()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### ✅ Grouping and Aggregation\n",
    "\n",
    "```python\n",
    "df.groupBy(\"Age\").count().show()\n",
    "df.groupBy(\"Age\").agg({\"Age\": \"avg\"}).show()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### ✅ Sorting\n",
    "\n",
    "```python\n",
    "df.sort(\"Age\").show()\n",
    "df.orderBy(df.Age.desc()).show()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### ✅ Joining DataFrames\n",
    "\n",
    "```python\n",
    "data1 = [(1, \"Ahmad\"), (2, \"Raza\")]\n",
    "data2 = [(1, \"Male\"), (2, \"Male\")]\n",
    "\n",
    "df1 = spark.createDataFrame(data1, [\"ID\", \"Name\"])\n",
    "df2 = spark.createDataFrame(data2, [\"ID\", \"Gender\"])\n",
    "\n",
    "df1.join(df2, on=\"ID\", how=\"inner\").show()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## 🧠 When to Use DataFrames Over RDDs\n",
    "\n",
    "| Feature         | DataFrame                           | RDD                           |\n",
    "| --------------- | ----------------------------------- | ----------------------------- |\n",
    "| Optimization    | Catalyst Optimizer                  | Manual                        |\n",
    "| Ease of use     | SQL-like operations                 | Functional transformations    |\n",
    "| Performance     | Fast and efficient                  | Slower for complex processing |\n",
    "| Structured data | Best for structured/semi-structured | Not suitable for tabular data |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fd1406",
   "metadata": {},
   "source": [
    "## Select, filter, where, withColumn, drop, distinct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dc9653",
   "metadata": {},
   "source": [
    "## ✅ 1. `select()`\n",
    "\n",
    "### ▶ Purpose: Selects specific columns from a DataFrame.\n",
    "\n",
    "```python\n",
    "df = spark.createDataFrame(\n",
    "    [(\"Ahmad\", 22), (\"Raza\", 25)],\n",
    "    [\"Name\", \"Age\"]\n",
    ")\n",
    "\n",
    "df.select(\"Name\").show()\n",
    "```\n",
    "\n",
    "📤 **Output:**\n",
    "\n",
    "```\n",
    "+-----+\n",
    "| Name|\n",
    "+-----+\n",
    "|Ahmad|\n",
    "| Raza|\n",
    "+-----+\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## ✅ 2. `filter()` / `where()`\n",
    "\n",
    "### ▶ Purpose: Filters rows based on a condition.\n",
    "\n",
    "These two methods are functionally **identical**.\n",
    "\n",
    "```python\n",
    "df.filter(df.Age > 22).show()\n",
    "df.where(df.Name == \"Ahmad\").show()\n",
    "```\n",
    "\n",
    "📤 **Output for filter:**\n",
    "\n",
    "```\n",
    "+----+---+\n",
    "|Name|Age|\n",
    "+----+---+\n",
    "|Raza| 25|\n",
    "+----+---+\n",
    "```\n",
    "\n",
    "📤 **Output for where:**\n",
    "\n",
    "```\n",
    "+-----+---+\n",
    "| Name|Age|\n",
    "+-----+---+\n",
    "|Ahmad| 22|\n",
    "+-----+---+\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## ✅ 3. `withColumn()`\n",
    "\n",
    "### ▶ Purpose: Adds a **new column** or **updates an existing one**.\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df.withColumn(\"AgePlus5\", col(\"Age\") + 5).show()\n",
    "```\n",
    "\n",
    "📤 **Output:**\n",
    "\n",
    "```\n",
    "+-----+---+--------+\n",
    "| Name|Age|AgePlus5|\n",
    "+-----+---+--------+\n",
    "|Ahmad| 22|      27|\n",
    "| Raza| 25|      30|\n",
    "+-----+---+--------+\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ 4. `drop()`\n",
    "\n",
    "### ▶ Purpose: Removes one or more columns.\n",
    "\n",
    "```python\n",
    "df.drop(\"Age\").show()\n",
    "```\n",
    "\n",
    "📤 **Output:**\n",
    "\n",
    "```\n",
    "+-----+\n",
    "| Name|\n",
    "+-----+\n",
    "|Ahmad|\n",
    "| Raza|\n",
    "+-----+\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## ✅ 5. `distinct()`\n",
    "\n",
    "### ▶ Purpose: Removes duplicate rows.\n",
    "\n",
    "```python\n",
    "df_dup = spark.createDataFrame(\n",
    "    [(\"Ahmad\", 22), (\"Ahmad\", 22), (\"Raza\", 25)],\n",
    "    [\"Name\", \"Age\"]\n",
    ")\n",
    "\n",
    "df_dup.distinct().show()\n",
    "```\n",
    "\n",
    "📤 **Output:**\n",
    "\n",
    "```\n",
    "+-----+---+\n",
    "| Name|Age|\n",
    "+-----+---+\n",
    "| Raza| 25|\n",
    "|Ahmad| 22|\n",
    "+-----+---+\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### ✅ Summary Table\n",
    "\n",
    "| Function       | Use Case                 | Example                         |\n",
    "| -------------- | ------------------------ | ------------------------------- |\n",
    "| `select()`     | Select specific columns  | `df.select(\"Name\")`             |\n",
    "| `filter()`     | Filter rows by condition | `df.filter(df.Age > 20)`        |\n",
    "| `where()`      | Same as `filter()`       | `df.where(df.Name == \"Ahmad\")`  |\n",
    "| `withColumn()` | Add/modify column        | `df.withColumn(\"new\", col + 1)` |\n",
    "| `drop()`       | Drop a column            | `df.drop(\"Age\")`                |\n",
    "| `distinct()`   | Remove duplicate rows    | `df.distinct()`                 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de42d5f",
   "metadata": {},
   "source": [
    "# Data types and schema inference?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c99f975",
   "metadata": {},
   "source": [
    "\n",
    "## ✅ What is a Schema in PySpark?\n",
    "\n",
    "A **schema** defines the structure of a DataFrame:\n",
    "\n",
    "* Column **names**\n",
    "* Column **data types**\n",
    "* Whether a column is **nullable**\n",
    "\n",
    "It is similar to a **table definition in SQL**.\n",
    "\n",
    "\n",
    "\n",
    "## 🧠 1. **Schema Inference**\n",
    "\n",
    "When creating a DataFrame, PySpark can **automatically infer the schema** by inspecting the data.\n",
    "\n",
    "### ✅ Example – Schema Inference\n",
    "\n",
    "```python\n",
    "data = [(\"Ahmad\", 22), (\"Raza\", 25)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "df.printSchema()\n",
    "```\n",
    "\n",
    "📤 **Output:**\n",
    "\n",
    "```\n",
    "root\n",
    " |-- Name: string (nullable = true)\n",
    " |-- Age: long (nullable = true)\n",
    "```\n",
    "\n",
    "PySpark infers:\n",
    "\n",
    "* `\"Name\"` is a `string`\n",
    "* `\"Age\"` is a `long` (integer)\n",
    "\n",
    "\n",
    "\n",
    "## 📘 2. **Manually Defining Schema**\n",
    "\n",
    "For better control, especially for large or structured datasets, define schema using `StructType` and `StructField`.\n",
    "\n",
    "### ✅ Example:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Age\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "data = [(\"Ahmad\", 22), (\"Raza\", 25)]\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.printSchema()\n",
    "```\n",
    "\n",
    "📤 **Output:**\n",
    "\n",
    "```\n",
    "root\n",
    " |-- Name: string (nullable = true)\n",
    " |-- Age: integer (nullable = true)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 Common PySpark Data Types\n",
    "\n",
    "| PySpark Type      | Description                     |\n",
    "| ----------------- | ------------------------------- |\n",
    "| `StringType()`    | String                          |\n",
    "| `IntegerType()`   | Integer (32-bit)                |\n",
    "| `LongType()`      | Long (64-bit)                   |\n",
    "| `FloatType()`     | Float (32-bit)                  |\n",
    "| `DoubleType()`    | Double precision float (64-bit) |\n",
    "| `BooleanType()`   | Boolean                         |\n",
    "| `DateType()`      | Date only                       |\n",
    "| `TimestampType()` | Date + Time                     |\n",
    "| `ArrayType()`     | Array/List                      |\n",
    "| `MapType()`       | Dictionary (key-value)          |\n",
    "| `StructType()`    | Nested row structure            |\n",
    "\n",
    "\n",
    "\n",
    "## ✅ Viewing Schema\n",
    "\n",
    "```python\n",
    "df.printSchema()  # Print schema\n",
    "df.schema         # Returns schema object\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## 🧪 Checking Data Types Programmatically\n",
    "\n",
    "```python\n",
    "for field in df.schema.fields:\n",
    "    print(f\"Column: {field.name}, Type: {field.dataType}\")\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## 📁 Schema Inference from CSV/JSON\n",
    "\n",
    "```python\n",
    "df = spark.read.csv(\"file.csv\", header=True, inferSchema=True)\n",
    "df.printSchema()\n",
    "```\n",
    "\n",
    "Use `inferSchema=True` to automatically detect data types.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kafka",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
