{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1065b05",
   "metadata": {},
   "source": [
    "# Running SQL queries on DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d4e14a",
   "metadata": {},
   "source": [
    "\n",
    "## âœ… Running SQL Queries on DataFrames\n",
    "\n",
    "PySpark allows you to write **SQL queries directly** on DataFrames by registering them as **temporary views**. This is especially useful if you're familiar with SQL and want to run queries on large distributed data.\n",
    "\n",
    "\n",
    "\n",
    "## ðŸ”„ Steps to Run SQL on DataFrames\n",
    "\n",
    "### 1. **Create or Load a DataFrame**\n",
    "\n",
    "```python\n",
    "data = [(\"Ahmad\", 22), (\"Raza\", 25), (\"Ali\", 25)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 2. **Register as a Temporary View**\n",
    "\n",
    "```python\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "```\n",
    "\n",
    "Now you can use SQL queries on `people`.\n",
    "\n",
    "\n",
    "\n",
    "### 3. **Run SQL Queries**\n",
    "\n",
    "```python\n",
    "result = spark.sql(\"SELECT Name FROM people WHERE Age > 22\")\n",
    "result.show()\n",
    "```\n",
    "\n",
    "ðŸ“¤ Output:\n",
    "\n",
    "```\n",
    "+-----+\n",
    "| Name|\n",
    "+-----+\n",
    "| Raza|\n",
    "|  Ali|\n",
    "+-----+\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## ðŸ“˜ SQL Functions & Types of Queries You Can Run\n",
    "\n",
    "### ðŸ”¹ SELECT Statements\n",
    "\n",
    "```python\n",
    "spark.sql(\"SELECT * FROM people\").show()\n",
    "```\n",
    "\n",
    "### ðŸ”¹ WHERE clause\n",
    "\n",
    "```python\n",
    "spark.sql(\"SELECT * FROM people WHERE Age = 25\").show()\n",
    "```\n",
    "\n",
    "### ðŸ”¹ GROUP BY\n",
    "\n",
    "```python\n",
    "spark.sql(\"SELECT Age, COUNT(*) as Count FROM people GROUP BY Age\").show()\n",
    "```\n",
    "\n",
    "### ðŸ”¹ ORDER BY\n",
    "\n",
    "```python\n",
    "spark.sql(\"SELECT * FROM people ORDER BY Age DESC\").show()\n",
    "```\n",
    "\n",
    "### ðŸ”¹ JOINS\n",
    "\n",
    "```python\n",
    "dept = [(\"Ahmad\", \"CS\"), (\"Raza\", \"IT\")]\n",
    "dept_df = spark.createDataFrame(dept, [\"Name\", \"Dept\"])\n",
    "dept_df.createOrReplaceTempView(\"departments\")\n",
    "\n",
    "joined = spark.sql(\"\"\"\n",
    "    SELECT p.Name, p.Age, d.Dept\n",
    "    FROM people p\n",
    "    JOIN departments d ON p.Name = d.Name\n",
    "\"\"\")\n",
    "joined.show()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## ðŸ”§ SQL-Specific Functions in PySpark\n",
    "\n",
    "| Function                    | Description                  |\n",
    "| --------------------------- | ---------------------------- |\n",
    "| `createOrReplaceTempView()` | Creates a temporary SQL view |\n",
    "| `sql()`                     | Executes SQL query string    |\n",
    "| `cacheTable(\"table\")`       | Caches a table in memory     |\n",
    "| `uncacheTable(\"table\")`     | Removes cached table         |\n",
    "| `isCached(\"table\")`         | Checks if a table is cached  |\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Example: Using SQL Functions\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "df.groupBy(\"Age\").agg(avg(\"Age\")).show()\n",
    "```\n",
    "\n",
    "Equivalent in SQL:\n",
    "\n",
    "```python\n",
    "spark.sql(\"SELECT Age, AVG(Age) FROM people GROUP BY Age\").show()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## ðŸ”„ Global vs Temporary Views\n",
    "\n",
    "| View Type                | Scope                    | Use                                 |\n",
    "| ------------------------ | ------------------------ | ----------------------------------- |\n",
    "| `createTempView()`       | Session-local            | Disappears after SparkSession ends  |\n",
    "| `createGlobalTempView()` | Global (across sessions) | Access with `global_temp.tablename` |\n",
    "\n",
    "\n",
    "\n",
    "### ðŸ“ Best Practice:\n",
    "\n",
    "Use SQL for complex filtering, joining, and aggregation â€” especially if collaborating with analysts or SQL-savvy team members.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b973681a",
   "metadata": {},
   "source": [
    "# Temporary Views and Global Views"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561bfdcd",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## âœ… 1. What is a View in PySpark?\n",
    "\n",
    "A **View** is like a **virtual table** created from a DataFrame. It lets you **query the DataFrame using SQL syntax**.\n",
    "\n",
    "There are two main types of views:\n",
    "\n",
    "| View Type             | Scope               | Accessible In          |\n",
    "|-----------------------|---------------------|------------------------|\n",
    "| Temporary View        | Current Session Only| Current SparkSession   |\n",
    "| Global Temporary View | All Sessions        | All SparkSessions via `global_temp` DB |\n",
    "\n",
    "\n",
    "\n",
    "## ðŸ§© 2. Temporary Views\n",
    "\n",
    "### â–¶ Description:\n",
    "- Registered for the **current session only**\n",
    "- Removed automatically when the session ends\n",
    "\n",
    "### âœ… Creating a Temporary View\n",
    "\n",
    "```python\n",
    "df = spark.createDataFrame([(\"Ahmad\", 22), (\"Raza\", 25)], [\"Name\", \"Age\"])\n",
    "\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "```\n",
    "\n",
    "### âœ… Running SQL Query\n",
    "\n",
    "```python\n",
    "spark.sql(\"SELECT * FROM people WHERE Age > 22\").show()\n",
    "```\n",
    "\n",
    "ðŸ“¤ Output:\n",
    "```\n",
    "+-----+---+\n",
    "| Name|Age|\n",
    "+-----+---+\n",
    "|Raza | 25|\n",
    "+-----+---+\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## ðŸŒ 3. Global Temporary Views\n",
    "\n",
    "### â–¶ Description:\n",
    "- **Persist across SparkSessions** and notebooks\n",
    "- Stored in the system database: `global_temp`\n",
    "\n",
    "### âœ… Creating a Global Temp View\n",
    "\n",
    "```python\n",
    "df.createOrReplaceGlobalTempView(\"people\")\n",
    "```\n",
    "\n",
    "### âœ… Accessing Global Temp View\n",
    "\n",
    "```python\n",
    "spark.sql(\"SELECT * FROM global_temp.people\").show()\n",
    "```\n",
    "\n",
    "> You must prefix the table name with `global_temp.`\n",
    "\n",
    "\n",
    "\n",
    "## âš™ï¸ Comparison Table\n",
    "\n",
    "| Feature               | Temporary View               | Global Temp View                   |\n",
    "|------------------------|------------------------------|------------------------------------|\n",
    "| Scope                 | Current SparkSession         | All SparkSessions                  |\n",
    "| Database Namespace    | `default` (no prefix)        | Must use `global_temp.` prefix     |\n",
    "| Lifecycle             | Ends with session            | Ends with Spark application        |\n",
    "| Use Case              | Session-limited operations   | Shared access across sessions      |\n",
    "\n",
    "\n",
    "\n",
    "## âœ… Functions to Manage Views\n",
    "\n",
    "| Function                            | Description                            |\n",
    "|-------------------------------------|----------------------------------------|\n",
    "| `createTempView(name)`              | Creates a temporary view               |\n",
    "| `createOrReplaceTempView(name)`     | Creates or replaces a temp view        |\n",
    "| `createGlobalTempView(name)`        | Creates a global temp view             |\n",
    "| `createOrReplaceGlobalTempView(name)`| Creates or replaces a global view      |\n",
    "| `spark.catalog.dropTempView(name)`  | Drops a temp view                      |\n",
    "| `spark.catalog.listTables()`        | Lists current session views/tables     |\n",
    "\n",
    "\n",
    "\n",
    "### ðŸ“ Best Practice:\n",
    "- Use **Temporary Views** for ad-hoc, session-specific SQL.\n",
    "- Use **Global Temp Views** for cross-session access and multi-user environments like Databricks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d223537",
   "metadata": {},
   "source": [
    "# SQL Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4ada9e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# âœ… PySpark SQL Functions List with Examples\n",
    "\n",
    "To use SQL functions:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import *\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## ðŸ”¹ 1. **Aggregate Functions**\n",
    "\n",
    "| Function  | Description   | Example                    |\n",
    "| --------- | ------------- | -------------------------- |\n",
    "| `count()` | Count rows    | `df.select(count(\"*\"))`    |\n",
    "| `sum()`   | Sum values    | `df.select(sum(\"salary\"))` |\n",
    "| `avg()`   | Average value | `df.select(avg(\"age\"))`    |\n",
    "| `min()`   | Minimum value | `df.select(min(\"age\"))`    |\n",
    "| `max()`   | Maximum value | `df.select(max(\"age\"))`    |\n",
    "\n",
    "```python\n",
    "df.groupBy(\"department\").agg(avg(\"salary\"), max(\"salary\")).show()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## ðŸ”¹ 2. **String Functions**\n",
    "\n",
    "| Function           | Description                    | Example                                         |\n",
    "| ------------------ | ------------------------------ | ----------------------------------------------- |\n",
    "| `lower()`          | Convert to lowercase           | `df.select(lower(\"name\"))`                      |\n",
    "| `upper()`          | Convert to uppercase           | `df.select(upper(\"name\"))`                      |\n",
    "| `length()`         | String length                  | `df.select(length(\"name\"))`                     |\n",
    "| `substr()`         | Substring                      | `df.select(substr(\"name\", 1, 3))`               |\n",
    "| `concat()`         | Concatenate strings            | `df.select(concat(col(\"fname\"), col(\"lname\")))` |\n",
    "| `trim()`           | Remove leading/trailing spaces | `df.select(trim(\"name\"))`                       |\n",
    "| `regexp_replace()` | Replace using regex            | `df.select(regexp_replace(\"name\", \"a\", \"@\"))`   |\n",
    "\n",
    "```python\n",
    "df.select(upper(\"name\").alias(\"NAME\"), length(\"name\")).show()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## ðŸ”¹ 3. **Date & Time Functions**\n",
    "\n",
    "| Function              | Description                | Example                                               |\n",
    "| --------------------- | -------------------------- | ----------------------------------------------------- |\n",
    "| `current_date()`      | Current date               | `df.select(current_date())`                           |\n",
    "| `current_timestamp()` | Current timestamp          | `df.select(current_timestamp())`                      |\n",
    "| `datediff()`          | Difference between 2 dates | `df.select(datediff(col(\"end\"), col(\"start\")))`       |\n",
    "| `months_between()`    | Months between 2 dates     | `df.select(months_between(col(\"end\"), col(\"start\")))` |\n",
    "| `add_months()`        | Add months to date         | `df.select(add_months(col(\"start\"), 2))`              |\n",
    "| `date_add()`          | Add days to date           | `df.select(date_add(col(\"start\"), 5))`                |\n",
    "| `date_sub()`          | Subtract days from date    | `df.select(date_sub(col(\"start\"), 3))`                |\n",
    "\n",
    "```python\n",
    "df = spark.createDataFrame([(\"2024-01-01\",)], [\"start\"])\n",
    "df.select(current_date(), datediff(current_date(), col(\"start\"))).show()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## ðŸ”¹ 4. **Null Handling Functions**\n",
    "\n",
    "| Function       | Description               | Example                             |\n",
    "| -------------- | ------------------------- | ----------------------------------- |\n",
    "| `isnull()`     | Check for null values     | `df.filter(col(\"age\").isNull())`    |\n",
    "| `isnotnull()`  | Check for non-null        | `df.filter(col(\"age\").isNotNull())` |\n",
    "| `fillna()`     | Replace null with a value | `df.fillna(0)`                      |\n",
    "| `dropna()`     | Drop rows with nulls      | `df.dropna()`                       |\n",
    "| `na.replace()` | Replace specific values   | `df.na.replace(\"NA\", None)`         |\n",
    "\n",
    "\n",
    "\n",
    "## ðŸ”¹ 5. **Conditional Functions**\n",
    "\n",
    "| Function | Description              | Example                                                        |\n",
    "| -------- | ------------------------ | -------------------------------------------------------------- |\n",
    "| `when()` | SQL CASE WHEN equivalent | `df.select(when(col(\"age\") > 18, \"Adult\").otherwise(\"Minor\"))` |\n",
    "| `expr()` | Run SQL expression       | `df.select(expr(\"age * 2\"))`                                   |\n",
    "| `col()`  | Reference column         | `df.select(col(\"age\") + 1)`                                    |\n",
    "\n",
    "```python\n",
    "df.select(\"name\", when(col(\"age\") >= 18, \"Adult\").otherwise(\"Minor\").alias(\"status\")).show()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## ðŸ”¹ 6. **Math Functions**\n",
    "\n",
    "| Function  | Description       | Example                         |\n",
    "| --------- | ----------------- | ------------------------------- |\n",
    "| `round()` | Round number      | `df.select(round(\"salary\", 2))` |\n",
    "| `sqrt()`  | Square root       | `df.select(sqrt(\"salary\"))`     |\n",
    "| `abs()`   | Absolute value    | `df.select(abs(\"salary\"))`      |\n",
    "| `exp()`   | Exponential       | `df.select(exp(\"age\"))`         |\n",
    "| `log()`   | Natural logarithm | `df.select(log(\"salary\"))`      |\n",
    "| `pow()`   | Power             | `df.select(pow(\"age\", 2))`      |\n",
    "\n",
    "\n",
    "\n",
    "## ðŸ”¹ 7. **Array & Collection Functions**\n",
    "\n",
    "| Function    | Description             | Example                                    |\n",
    "| ----------- | ----------------------- | ------------------------------------------ |\n",
    "| `split()`   | Split string into array | `df.select(split(\"name\", \" \"))`            |\n",
    "| `explode()` | Flatten array into rows | `df.select(explode(split(\"skills\", \",\")))` |\n",
    "| `array()`   | Create array            | `df.select(array(\"col1\", \"col2\"))`         |\n",
    "| `size()`    | Size of array           | `df.select(size(split(\"skills\", \",\")))`    |\n",
    "\n",
    "\n",
    "\n",
    "## ðŸ”¹ 8. **Window Functions** (used with `Window` spec)\n",
    "\n",
    "| Function       | Description        | Example                              |\n",
    "| -------------- | ------------------ | ------------------------------------ |\n",
    "| `rank()`       | SQL Rank           | `rank().over(windowSpec)`            |\n",
    "| `dense_rank()` | Dense rank         | `dense_rank().over(windowSpec)`      |\n",
    "| `row_number()` | Row number         | `row_number().over(windowSpec)`      |\n",
    "| `lag()`        | Previous row value | `lag(\"salary\", 1).over(windowSpec)`  |\n",
    "| `lead()`       | Next row value     | `lead(\"salary\", 1).over(windowSpec)` |\n",
    "\n",
    "```python\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "w = Window.partitionBy(\"dept\").orderBy(\"salary\")\n",
    "df.withColumn(\"rank\", rank().over(w)).show()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## âœ… Bonus: Use `expr()` to Run SQL Inside DataFrame API\n",
    "\n",
    "```python\n",
    "df.selectExpr(\"name\", \"age * 2 as double_age\").show()\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0360725",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# âœ… `groupBy()`, `agg()`, and Advanced Aggregations in PySpark\n",
    "\n",
    "These operations are used to **group data** and perform **aggregate computations** just like SQL's `GROUP BY`.\n",
    "\n",
    "\n",
    "\n",
    "## ðŸ”¹ 1. `groupBy()`\n",
    "\n",
    "Groups rows based on column(s).\n",
    "\n",
    "### âœ… Example:\n",
    "\n",
    "```python\n",
    "df.groupBy(\"department\").count().show()\n",
    "```\n",
    "\n",
    "ðŸ“¤ Output:\n",
    "\n",
    "```\n",
    "+----------+-----+\n",
    "|department|count|\n",
    "+----------+-----+\n",
    "|     Sales|    3|\n",
    "|       HR |    2|\n",
    "+----------+-----+\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## ðŸ”¹ 2. `agg()` â€“ Perform Multiple Aggregations\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import avg, max, min\n",
    "\n",
    "df.groupBy(\"department\").agg(\n",
    "    avg(\"salary\").alias(\"avg_salary\"),\n",
    "    max(\"salary\").alias(\"max_salary\")\n",
    ").show()\n",
    "```\n",
    "\n",
    "ðŸ“¤ Output:\n",
    "\n",
    "```\n",
    "+----------+----------+----------+\n",
    "|department|avg_salary|max_salary|\n",
    "+----------+----------+----------+\n",
    "|     Sales|   45000.0|     60000|\n",
    "|       HR |   35000.0|     40000|\n",
    "+----------+----------+----------+\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## ðŸ”¹ 3. Group by Multiple Columns\n",
    "\n",
    "```python\n",
    "df.groupBy(\"department\", \"gender\").count().show()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## ðŸ”¹ 4. Window Aggregations (Advanced)\n",
    "\n",
    "Useful when you want to keep row-level details **with aggregated metrics**.\n",
    "\n",
    "```python\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "windowSpec = Window.partitionBy(\"department\")\n",
    "\n",
    "df.withColumn(\"avg_salary\", avg(\"salary\").over(windowSpec)).show()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## ðŸ”¹ 5. Pivot Table (Pivoting Data)\n",
    "\n",
    "```python\n",
    "df.groupBy(\"department\").pivot(\"gender\").agg(avg(\"salary\")).show()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## ðŸ”¹ Common Aggregation Functions\n",
    "\n",
    "| Function         | Description                   |\n",
    "| ---------------- | ----------------------------- |\n",
    "| `count()`        | Number of rows                |\n",
    "| `sum()`          | Sum of values                 |\n",
    "| `avg()`          | Average value                 |\n",
    "| `min()`          | Minimum value                 |\n",
    "| `max()`          | Maximum value                 |\n",
    "| `mean()`         | Mean value (alias of avg)     |\n",
    "| `collect_list()` | Aggregates values into a list |\n",
    "| `collect_set()`  | Aggregates distinct values    |\n",
    "\n",
    "\n",
    "\n",
    "### âœ… Example: collect\\_list and collect\\_set\n",
    "\n",
    "```python\n",
    "df.groupBy(\"department\").agg(collect_list(\"name\")).show()\n",
    "```\n",
    "\n",
    "ðŸ“¤ Output:\n",
    "\n",
    "```\n",
    "+----------+---------------------+\n",
    "|department|collect_list(name)  |\n",
    "+----------+---------------------+\n",
    "|   HR     | [John, Alice]       |\n",
    "|   Sales  | [Bob, Eve, Charlie] |\n",
    "+----------+---------------------+\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48db0141",
   "metadata": {},
   "source": [
    "#  Window functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19ab0d7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# âœ… Window Functions in PySpark\n",
    "\n",
    "**Window functions** allow you to perform **aggregations and calculations across a window (group of rows)** related to the current row **without collapsing the rows** like `groupBy()` does.\n",
    "\n",
    "\n",
    "\n",
    "## ðŸ”¸ Why Use Window Functions?\n",
    "\n",
    "* To compute **rankings**, **running totals**, **percentiles**, **previous/next row comparisons** (like SQL's `LEAD`/`LAG`)\n",
    "* To **aggregate over partitions** of data while preserving row-level detail\n",
    "\n",
    "\n",
    "\n",
    "## ðŸ”¸ Step-by-Step Usage\n",
    "\n",
    "```python\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Define a window spec\n",
    "windowSpec = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## ðŸ”¸ Common Window Functions\n",
    "\n",
    "| Function         | Description                             |\n",
    "| ---------------- | --------------------------------------- |\n",
    "| `row_number()`   | Unique row number in a window partition |\n",
    "| `rank()`         | Ranking with gaps                       |\n",
    "| `dense_rank()`   | Ranking without gaps                    |\n",
    "| `lag()`          | Previous row value                      |\n",
    "| `lead()`         | Next row value                          |\n",
    "| `ntile(n)`       | Bucket rows into `n` equal groups       |\n",
    "| `sum()`, `avg()` | Rolling aggregations                    |\n",
    "| `min()`, `max()` | Rolling min/max                         |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Example DataFrame\n",
    "\n",
    "```python\n",
    "data = [\n",
    "    (\"Ahmad\", \"Sales\", 60000),\n",
    "    (\"Raza\", \"Sales\", 50000),\n",
    "    (\"Ali\", \"Sales\", 40000),\n",
    "    (\"Sara\", \"HR\", 45000),\n",
    "    (\"John\", \"HR\", 50000),\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"name\", \"department\", \"salary\"])\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## ðŸ”¹ 1. `row_number()` Example\n",
    "\n",
    "```python\n",
    "windowSpec = Window.partitionBy(\"department\").orderBy(col(\"salary\").desc())\n",
    "\n",
    "df.withColumn(\"row_num\", row_number().over(windowSpec)).show()\n",
    "```\n",
    "\n",
    "ðŸ“¤ Output:\n",
    "\n",
    "```\n",
    "+-----+----------+------+--------+\n",
    "|name |department|salary|row_num|\n",
    "+-----+----------+------+--------+\n",
    "|Ahmad|Sales     |60000 |1      |\n",
    "|Raza |Sales     |50000 |2      |\n",
    "|Ali  |Sales     |40000 |3      |\n",
    "|John |HR        |50000 |1      |\n",
    "|Sara |HR        |45000 |2      |\n",
    "+-----+----------+------+--------+\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## ðŸ”¹ 2. `rank()` and `dense_rank()`\n",
    "\n",
    "```python\n",
    "df.withColumn(\"rank\", rank().over(windowSpec)).withColumn(\"dense_rank\", dense_rank().over(windowSpec)).show()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## ðŸ”¹ 3. `lag()` and `lead()` â€“ Previous & Next Row\n",
    "\n",
    "```python\n",
    "df.withColumn(\"prev_salary\", lag(\"salary\", 1).over(windowSpec)).withColumn(\"next_salary\", lead(\"salary\", 1).over(windowSpec)).show()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## ðŸ”¹ 4. Rolling Aggregation (sum, avg)\n",
    "\n",
    "```python\n",
    "windowSpecRows = Window.partitionBy(\"department\").orderBy(\"salary\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "df.withColumn(\"cumulative_salary\", sum(\"salary\").over(windowSpecRows)).show()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## ðŸ”¹ 5. `ntile(n)` â€“ Divide rows into `n` buckets\n",
    "\n",
    "```python\n",
    "df.withColumn(\"bucket\", ntile(2).over(windowSpec)).show()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## âœ… Summary: Creating and Using a Window Function\n",
    "\n",
    "```python\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Create window specification\n",
    "windowSpec = Window.partitionBy(\"dept\").orderBy(\"salary\")\n",
    "\n",
    "# Apply window function\n",
    "df.withColumn(\"rank\", rank().over(windowSpec)).show()\n",
    "```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
